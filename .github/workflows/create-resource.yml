name: Create Resources

on:
  push:
    paths:
      - 'resource.json'
  workflow_dispatch:

jobs:
  create-resources:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Install and Upgrade Python Packages
      run: |
        sudo apt-get install -y python3-pip
        pip3 install --upgrade "urllib3<2" "chardet<4" requests

    - name: Install Databricks CLI
      run: |
        pip3 install databricks-cli

    - name: Parse JSON file and save environment variables
      id: parse_json
      run: |
        python3 <<EOF
        import json

        with open('resource.json') as f:
            data = json.load(f)
        
        platform_type = data['PlatformType']
        num_resources = len(data['Resources'])
        
        # Save platform_type and num_resources for later steps
        with open('outputs/platform_type.txt', 'w') as f:
            f.write(platform_type)
        
        with open('outputs/num_resources.txt', 'w') as f:
            f.write(str(num_resources))
        
        # Save all resources individually for later processing
        for i, resource in enumerate(data['Resources']):
            with open(f'outputs/resource_{i}_type.txt', 'w') as f:
                f.write(resource['ResourceType'])
                
            with open(f'outputs/resource_{i}_name.txt', 'w') as f:
                f.write(resource['ResourceName'])
                
            with open(f'outputs/resource_{i}_config.json', 'w') as f:
                json.dump(resource, f)
        EOF

    - name: Load platform type and number of resources
      id: load_platform
      run: |
        platform_type=$(cat outputs/platform_type.txt)
        num_resources=$(cat outputs/num_resources.txt)
        
        echo "::set-output name=platform_type::$platform_type"
        echo "::set-output name=num_resources::$num_resources"
        
        echo "Platform type is $platform_type"
        echo "Number of resources is $num_resources"

    - name: Configure Terraform
      uses: hashicorp/setup-terraform@v1
      with:
        terraform_version: 1.0.11
        
    - name: Create AWS Resources
      if: steps.load_platform.outputs.platform_type == 'AWS'
      run: |
        num_resources=$(cat outputs/num_resources.txt)
        rm -f main.tf  # Ensure main.tf is clean before starting

        for i in $(seq 0 $((num_resources-1))); do
          resource_type=$(cat outputs/resource_${i}_type.txt)
          resource_name=$(cat outputs/resource_${i}_name.txt)
          resource_config=$(cat outputs/resource_${i}_config.json)

          echo "Creating $resource_type named $resource_name"

          case $resource_type in
            S3)
              echo "resource \"aws_s3_bucket\" \"$resource_name\" {
                bucket = \"$resource_name\"
              }" >> main.tf
              ;;
            EC2)
              ami=$(echo $resource_config | python3 -c 'import sys, json; print(json.load(sys.stdin)["Ami"])')
              instance_type=$(echo $resource_config | python3 -c 'import sys, json; print(json.load(sys.stdin)["InstanceType"])')
              echo "resource \"aws_instance\" \"$resource_name\" {
                ami           = \"$ami\"
                instance_type = \"$instance_type\"
              }" >> main.tf
              ;;
            SNS)
              echo "resource \"aws_sns_topic\" \"$resource_name\" {
                name = \"$resource_name\"
              }" >> main.tf
              ;;
            *)
              echo "Unsupported resource type: $resource_type"
              exit 1
              ;;
          esac
        done

        terraform init
        terraform apply -auto-approve

    - name: Process Databricks
      if: steps.load_platform.outputs.platform_type == 'DATABRICK'
      run: |
        num_resources=$(cat outputs/num_resources.txt)

        for i in $(seq 0 $((num_resources-1))); do
          resource_type=$(cat outputs/resource_${i}_type.txt)
          resource_name=$(cat outputs/resource_${i}_name.txt)
          resource_config=$(cat outputs/resource_${i}_config.json)

          echo "Creating Databricks resource: Type=$resource_type, Name=$resource_name"

          case $resource_type in
            CLUSTER)
              cluster_config=$(echo $resource_config | python3 -c 'import sys, json; print(json.load(sys.stdin)["ClusterConfig"])')
              echo "Setting up Databricks Cluster ${resource_name} with config ${cluster_config}"
              databricks clusters create --json "$cluster_config"
              ;;
            JOB)
              job_config=$(echo $resource_config | python3 -c 'import sys, json; print(json.load(sys.stdin)["JobConfig"])')
              echo "Setting up Databricks Job ${resource_name} with config ${job_config}"
              databricks jobs create --json "$job_config"
              ;;
            *)
              echo "Unsupported Databricks resource type: $resource_type"
              exit 1
              ;;
          esac
        done

    - name: Process Snowflake
      if: steps.load_platform.outputs.platform_type == 'DATAFLAKE'
      run: |
        echo "Processing Snowflake resources..."
        num_resources=$(cat outputs/num_resources.txt)

        for i in $(seq 0 $((num_resources-1))); do
          resource_type=$(cat outputs/resource_${i}_type.txt)
          resource_name=$(cat outputs/resource_${i}_name.txt)
          resource_config=$(cat outputs/resource_${i}_config.json)

          echo "Creating Snowflake resource: Type=$resource_type, Name=$resource_name"

          # Add your Snowflake related commands here based on the resource_type and resource_config
          
        done
